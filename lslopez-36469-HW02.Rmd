---
title: "36-469 Homework 02, Fall 2021"
author: "Leandro Lopez Leon (lslopez)"
date: "Monday, Sept. 27, 2021" 
output:
  pdf_document:
    toc: no
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
---

```{r wrap-hook,echo=FALSE}
    library(knitr)
    library(tidyverse)
    hook_output = knit_hooks$get('output')
    knitr::opts_chunk$set(echo = F, warning = F, message = F)
    knit_hooks$set(output = function(x, options) {
      # this hook is used only when the linewidth option is not NULL
      if (!is.null(n <- options$linewidth)) {
        x = knitr:::split_lines(x)
        # any lines wider than n should be wrapped
        if (any(nchar(x) > n)) x = strwrap(x, width = n)
        x = paste(x, collapse = '\n')
      }
      hook_output(x, options)
    })
```

## Q1
**A**.  

*Compute the OR for actn3_rs540874*
```{r}
    source("https://raw.githubusercontent.com/xuranw/469_public/master/hw2/hw2_functions.R")
    famuss <- read.csv("https://raw.githubusercontent.com/xuranw/469_public/master/hw2/synthetic_famuss.csv")
    or_df <- famuss %>% subset(., actn3_rs540874 != 2) %>%
             select(actn3_rs540874, heart_disease)
    D_A <- table(or_df)[2,2]
    H_A <- table(or_df)[2,1]
    D_a <- table(or_df)[1,2]
    H_a <- table(or_df)[1,1]
    (D_A / H_A) /  (D_a / H_a)
```
*Interpretation*
Observe that $OR \approx 1$, which suggests that the risk factor `actn3_rs540874` (in isolation) is not significantly associated with the heart disease. 

**B**.  
*Use logistic regression with the glm function to regress only heart disease onto all 79 SNP’s (i.e., not including the variable `norm_BMI`). Plot these approximate OR’s*

```{r}
    logistic_regression  <- function(dataset, BMI) {
        dataset <- dataset[order(names(dataset))]
        glm_res <- stats::glm(heart_disease ~ ., data = dataset, family = stats::binomial)
        pred_vec1 <-  stats::predict(glm_res, newx = dataset, type = "response")
        pred_vec1 <- as.numeric(pred_vec1 >= 0.5) # converting it to a 0-1
        misclassification_rate  <- 1 - sum(dataset$heart_disease == pred_vec1) / length (dataset$heart_disease)
        b_js <- output_coefficients(glm_res, dataset, which(colnames(glm_famuss) == "heart_disease"))
        b_js_reduced <- b_js[which(names(b_js) != "norm_BMI" & names(b_js) != "(Intercept)")]
        ors <- data.frame(as.vector(unlist(lapply(b_js_reduced, exp))), 1:length(b_js_reduced))
        names(ors) <- c("ORs", "x")
        p <- ggplot(data = ors, ggplot2::aes(x = x, y = ORs)) + geom_point() +
        geom_hline(yintercept = .8, linetype = "dotted", col = "red") +
        geom_hline(yintercept = 1, col = "red") +
        geom_hline(yintercept = 1.2,linetype = "dotted", col = "red") +
        labs(x = "SNP order (alphabetical)",
             y = "Estimated OR",
             title = ifelse(BMI == T, "Regression (with BMI)", "Regression (without BMI)"),
             subtitle = paste("Misclassification: ", misclassification_rate))
        result  <- list(bmi = ifelse(BMI == T, b_js["norm_BMI"], NA), p.plot = p)
        return(result)
    }
    glm_famuss <- famuss[, -which(colnames(famuss) == "norm_BMI")]
    logistic_regression(glm_famuss, F)[2]
```
**C**.  
*We now include `norm_BMI`. The magnitude of coefficient associated with `norm_BMI`* is
```{r}
    logistic_regression(famuss, T)
```
*Similarities*: Both fits place the odd ratios for most SNP's inside the dotted lines at $[0.8, 1.2]$. There also seems
to be some overlap in the SNPs with high magnitude in both graphs. 

*Differences*: Most remarkably, the misclassification rate went down considerably with the addition of `norm_BMI`. There are also a few
SNP's with large OR's which where unremarkable in the first graph. 

**D**.  

```{r}
    library(glmnet)
    set.seed(10)
    Y <-  famuss[,which(colnames(famuss) == "heart_disease")]
    X <-  famuss[,which(colnames(famuss) != "heart_disease")]
    cvglmnet_res <- cv.glmnet(x = as.matrix(X), y = as.matrix(Y),
                             family = "binomial", intercept = T, alpha = 1)
    coef_vec1 <- stats::coef(cvglmnet_res, s = "lambda.1se")
    name_vec <- rownames(coef_vec1)
    coef_vec1 <- as.numeric(coef_vec1)
    names(coef_vec1) <- name_vec
    coef_vec1 <- coef_vec1[order(names(coef_vec1))]
    coef_vec1_reduced <- coef_vec1[which(names(coef_vec1) != "(Intercept)" & names(coef_vec1) != "norm_BMI")]
    pred_vec <- output_predictions(cvglmnet_res, famuss, which(colnames(famuss) == "heart_disease"))
    ors <- data.frame(as.vector(unlist(lapply(coef_vec1_reduced, exp))), 1:length(coef_vec1_reduced))
    names(ors) <- c("ORs", "x")
    misclassification_rate  <- 1 - sum(famuss$heart_disease == pred_vec) / length(famuss$heart_disease)
    ggplot(data = ors, ggplot2::aes(x = x, y = ORs)) +
            geom_point() +
            geom_hline(yintercept = .8, linetype = "dotted", col = "red") +
            geom_hline(yintercept = 1, col = "red") +
            geom_hline(yintercept = 1.2,linetype = "dotted", col = "red") + 
            labs(x = "SNP order (alphabetical)",
                 y = "Estimated OR",
                 title = "Penalized Logistic Regression (with BMI)", 
                 subtitle = paste("Misclassification: ", misclassification_rate))
    coef_vec1["norm_BMI"]
```
*Similarities*: None  

*Differences*: Although the misclassification rate went up slightly in the Figure 3, it did so with tremendous parsimony in the model (only 3 
ORs not equal to 1). The magnitude of the OR was also pushed towards 1 aggresively in Figure 3 due to the high $\alpha$ value.  
Interestingly, the non-unity SNP's in Figure 3 seem to be different the largest most significant ones in Figure 2 (not exactly sure about this).

## Q2
**A**.  
*Inputs*: $n$ (representing sample size) and $p$ (representing covariates), $k$ which splits up `cor_mat`into equally shaped submatrices, and `core_within` which is the correlation between variables 
$x _ i , x _ j$ where $i,j$ depend on $k$ since not all variables are correlated with each other, only 
those inside the same submatrix

*Description*: We create correlation matrix which is partitioned into $k$ submatrices, where each covariate 
$X _ j$ inside submatrix $M _ l$ is correlated to all covariates $X _ i, i \neq j$, which are also 
inside $M _ l$. This creates collinearity between the covariates inside each of the sub-matrices. 
Then we sample an i.i.d Gaussian training set $x \in \mathbb{R}^{n \times p}$ with $n$ subjects and $p$ covariates 
where $\mu$ is the zero vector and $\sigma$ is our devilishly constructed `cor_mat`. Finally, we create biased set of coefficients 
$B _ j$ and generate a random response $y$ by applying these coefficients plus a random error.  

*Outputs*: The output is a triple $(x,y,B)$, where $x$ is described above, $y$ is the response generated by the operation 
$xB + \epsilon, \epsilon \sim \mathcal{N}(0,1)$. 

**B**.  
```{r}
    library(reshape2)
    cors <- c(0, 0.5, 0.9)
    for (correlation in cors) {
        L <- generate_data(n = 1000, p = 100, cor_within = correlation)
        melted_x <- melt(cor(L$x))
        p <- ggplot(melted_x, aes(x=Var2, y=Var1, fill=value)) + geom_tile() + labs(title = paste("Correlation: ", correlation))
        print(p)
    }
```
We note that since $k=3$, there are 3 submatrices whose correlation increases as `cor_within` increases. since the off diagonal terms inside 
these submatrices are all equal to `cor_within`.

**C**.  
```{r}
    simulate_lasso <- function(n, cor_within) { 

    }
```


