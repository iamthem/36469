---
title: "36-469 Homework 04, Fall 2021"
author: "Leandro Lopez Leon (lslopez)"
date: "Monday, Oct. 25, 2021 (one day late)" 
output:
  pdf_document:
    toc: no
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
---

```{r wrap-hook,echo=FALSE}
    library(knitr)
    library(tidyverse)
    hook_output = knit_hooks$get('output')
    knitr::opts_chunk$set(echo = F, warning = F, message = F)
    knit_hooks$set(output = function(x, options) {
      # this hook is used only when the linewidth option is not NULL
      if (!is.null(n <- options$linewidth)) {
        x = knitr:::split_lines(x)
        # any lines wider than n should be wrapped
        if (any(nchar(x) > n)) x = strwrap(x, width = n)
        x = paste(x, collapse = '\n')
      }
      hook_output(x, options)
    })
```

## Q1
```{r}
    library(tidyverse)
    library(gridExtra)
    snp_data <- as.matrix(read.csv("https://raw.githubusercontent.com/xuranw/469_public/master/hw4/synthetic_famuss.csv"))
    heart_disease <- snp_data[,1]; 
    snp_data <- snp_data[,-1]
    set.seed(10)
    n <- length(heart_disease)
    idx <- sample(1:n, round(.2*n))
    train_dat <- snp_data[-idx,]; train_label <- heart_disease[-idx]
    test_dat <- snp_data[idx,]; test_label <- heart_disease[idx]
```
**A**.  
```{r}
    pca_res <- stats::prcomp(snp_data, center = T, scale. = T)
    pc1and2 <- pca_res$rotation[,c(1,2)]
    coordinates_train <- data.frame(cbind(train_dat %*% pc1and2, heart_disease))
    coordinates_test <- data.frame(cbind(test_dat %*% pc1and2, heart_disease))
    plot_PCs <- function(coordinates, plot_title, fact) {
        g  <- ggplot(coordinates, aes(x = PC1, y = PC2, color = as.factor(fact))) +
                 geom_point() +
                 labs(x = "Principal Component 1",
                 y = "Principal Component 2", color = "Has CHD?", title = plot_title)
        return(g)
    }
    g_train <- plot_PCs(coordinates_train, "Training Data: Observed", coordinates_train$heart_disease)
    g_test <- plot_PCs(coordinates_test, "Testing Data: Observed", coordinates_test$heart_disease)
    grid.arrange(g_train, g_test, ncol = 2)
```

We note a correspondence between our training and test data sets along the first two principal components. Although 
the training data is more numerous, the plot sugggest both samples are coming from a similar data generating process. 

**B**.  

```{r}
    library(glmnet)
    source("https://raw.githubusercontent.com/xuranw/469_public/master/hw2/hw2_functions.R")
    cvglmnet_res <- cv.glmnet(x = as.matrix(train_dat), y = as.matrix(train_label),
                             family = "binomial", intercept = T, alpha = 1)
    coef_vec1 <- stats::coef(cvglmnet_res, s = "lambda.1se")
    train_df <- data.frame(cbind(train_dat, train_label))
    test_df <- data.frame(cbind(test_dat, test_label))
    pred_vec_train <- as.numeric(predict(cvglmnet_res, train_dat, type = "response") >= 0.5)
    pred_vec_test <- as.numeric(predict(cvglmnet_res, test_dat, type = "response") >= 0.5)
    pred_error_train <- sum(pred_vec_train != train_label) / length(pred_vec_train)
    pred_error_test <- sum(pred_vec_test != test_label) / length(pred_vec_test)
    coordinates_train <- data.frame(cbind(train_dat %*% pc1and2, pred_vec_train))
    coordinates_test <- data.frame(cbind(test_dat %*% pc1and2, pred_vec_test))
    g_train <- plot_PCs(coordinates_train,
                        paste("Training Data: Logistic reg.\nTraining Error: ", round(pred_error_train, 2)),
                        coordinates_train$pred_vec_train)
    g_test <- plot_PCs(coordinates_test,
                       paste("Testing Data: Logistic reg.\nTesting Error: ", round(pred_error_test, 2)),
                       coordinates_test$pred_vec_test)
    grid.arrange(g_train, g_test, ncol = 2)
```

The logistic fit seems to neatly cluster our observations. We note that it classifies (most) points in the upper left hand corner as 0, 
and (most) points on the lower right as 1. The actual data is not so neat and has a more nuanced decision boundary which `glm` cannot approximate

**C**.  

```{r, results = 'hide'}
    library(xgboost)
    xgb_cv <-  xgboost::xgb.cv(data = train_dat, label = train_label,
                    nrounds = 20, nfold = 5, metrics = list("error"),
                    max_depth = 5, objective = "binary:logistic",
                    early_stopping_rounds = 5, verbose = F)
```

The optimal number of trees is given below 

```{r}
        xgb_cv$best_iteration
```

```{r, results = 'hide'}
    xgb_opt <-  xgboost::xgboost(data = train_dat, label = train_label,
                    nrounds = xgb_cv$best_iteration, metrics = list("error"),
                    max_depth = 5, objective = "binary:logistic",
                    early_stopping_rounds = 5, verbose = F)
    
    train_pred <- as.numeric(stats::predict(xgb_opt, newdata = train_dat) >= 0.5)
    test_pred <- as.numeric(stats::predict(xgb_opt, newdata = test_dat) >= 0.5)

    pred_error_train <- sum(train_pred != train_label) / length(train_pred)
    pred_error_test <- sum(test_pred != test_label) / length(test_pred)

    coordinates_train <- data.frame(cbind(train_dat %*% pc1and2, train_pred))
    coordinates_test <- data.frame(cbind(test_dat %*% pc1and2, test_pred))
```

```{r}
    g_train <- plot_PCs(coordinates_train,
                        paste("Training Data: XGBoost \nTraining Error: ", round(pred_error_train, 2)),
                        coordinates_train$train_pred)
    g_test <- plot_PCs(coordinates_test,
                       paste("Testing Data: XGBoost \nTesting Error: ", round(pred_error_test, 2)),
                       coordinates_test$test_pred)
    grid.arrange(g_train, g_test, ncol = 2)
```
In the figure above there is still clustering in the training and test sets. However, our decision boundary is no longer a line (like in the previous figure). 
Note that our model still classifies nearby points with the same label. Whereas in the true responses, there is much more intermixing of the two classes. 

**D**.  

```{r, include = F}
    fit_xgboost <- function(max_depth)  {
        xgb_out <- xgboost::xgboost(data = train_dat, label = train_label,
                        nrounds = xgb_cv$best_iteration, metrics = list("error"),
                        max_depth = max_depth, objective = "binary:logistic",
                        early_stopping_rounds = 5, verbose = F)
    
        train_pred <- as.numeric(stats::predict(xgb_out, newdata = train_dat) > 0.5)
        test_pred <- as.numeric(stats::predict(xgb_out, newdata = test_dat) > 0.5)

        pred_error_train <- sum(train_pred != train_label) / length(train_pred)
        pred_error_test <- sum(test_pred != test_label) / length(test_pred) 
        c(pred_error_train, pred_error_test, max_depth)
    }
    depths <- 1:10
    output <- sapply(depths, fit_xgboost, simplify = T)
```

```{r, include = F }
    x_axis <- as.vector(sapply(output[3,], function(x){c(x,x)}))
    output_df <- data.frame(cbind(as.vector(output[c(1,2),]),
                                  rep(c("Training Misclassification","Testing Misclassification"), 10),
                                  as.numeric(x_axis)))
    names(output_df) <- c("misclass", "label", "depth")
```
```{r}
    ggplot(data = output_df, aes(x = factor(depth, level = depths), y = misclass, group = label)) +
        geom_line(aes(linetype = label, color = label)) +
        labs(x = "Max Depth",
            y = "Misclassification",
            title = "Training vs. Testing Comparison")
```

Observe that training error decreases monotonically for depth $\in [1,7]$, while testing error dips and then increases. This suggests 
that our model is giving relevance to noise in the training sample a.k.a overfitting.  

**E**.  
```{r, include = F}
    xgb_out <- xgboost::xgboost(data = train_dat, label = train_label,
                    nrounds = xgb_cv$best_iteration, nfold = 5, metrics = list("error"),
                    max_depth = 3, objective = "binary:logistic",
                    early_stopping_rounds = 5, verbose = F)
    importance_mat <- xgboost::xgb.importance(model = xgb_out)
```

```{r}
    importance_mat[1:6,]
    xgboost::xgb.plot.importance(importance_mat)
```


## Q2

**A**.  

```{r}
    source("https://raw.githubusercontent.com/xuranw/469_public/master/hw4/hw4_functions.R")
    dat <- as.matrix(read.csv("https://raw.githubusercontent.com/xuranw/469_public/master/hw4/synthetic_data.csv"))
    y <- dat[,1]; x <- dat[,2:3]
    grid_val <- seq(-5, 5, length.out = 100)
    test_grid <- as.matrix(expand.grid(grid_val, grid_val))
    example_classifier <- function(vec){ ifelse(vec[2] >= 2, 0, 1)}
    pred_vec <- apply(test_grid, 1, example_classifier)
    plot_prediction_region(x, y, pred_vec, test_grid,
    xlab = "Dimension 1", ylab = "Dimension 2",
    main = "Example decision boundary",
    pch = 16, asp = T)
```

```{r}
    dataset <- data.frame(cbind(x,y))
    colnames(test_grid) <- c("x1", "x2")
    test_grid_df <- data.frame(test_grid)
    names(test_grid_df) <- c("x1", "x2")
    glm_res <- glm(y ~ ., data = dataset, family = stats::binomial)
    pred_vec <- as.numeric(predict(glm_res, test_grid_df, type = "response") >= 0.5)
    plot_prediction_region(x, y, pred_vec, test_grid,
    xlab = "Dimension 1", ylab = "Dimension 2",
    main = "Logistic reg. decision boundary",
    pch = 16, asp = T)
```

Observe that `glm` finds a separating hyperplane (line in 2D) which cuts through the middle of the 
data.

**B**.  

```{r, include = F }
    xgb_1 <-  xgboost::xgboost(data = x, label = y,
                    nrounds = 1, metrics = list("error"),
                    max_depth = 1, objective = "binary:logistic",
                    verbose = F)
    pred_vec <- as.numeric(predict(xgb_1, test_grid, type = "response") >= 0.5)
```

```{r}
    plot_prediction_region(x, y, pred_vec, test_grid,
    xlab = "Dimension 1", ylab = "Dimension 2",
    main = "XGBoost decision boundary\n1 trees, depth 3",
    pch = 16, asp = T)
```

For a single depth-3 tree, the boundary remains linear since we can only partition the space for one of the features (in this case 'dimension 2'). 

**C**.  

```{r, include = F }
        
    xgb_50 <-  xgboost::xgboost(data = x, label = y,
                    nrounds = 50, metrics = list("error"),
                    max_depth = 3, objective = "binary:logistic",
                    verbose = F)
```
```{r}
    pred_vec <- as.numeric(predict(xgb_50, test_grid, type = "response") >= 0.5)
    plot_prediction_region(x, y, pred_vec, test_grid,
    xlab = "Dimension 1", ylab = "Dimension 2",
    main = "XGBoost decision boundary\n50 trees, depth 3",
    pch = 16, asp = T)
```

We note that 50 tress of depth-3 result in a decision boundary which classifies the training data perfectly, but creates gaps which are not in the true decision boundary 

**D**.  

```{r, include = F }
    xgb_cv <-  xgboost::xgb.cv(data = x, label = y,
                    nrounds = 50, nfold = 5, metrics = list("error"),
                    max_depth = 3, objective = "binary:logistic",
                    early_stopping_rounds = 5,
                    verbose = F)
```
```{r, include = F }
    xgb_opt <-  xgboost::xgboost(data = x, label = y,
                    nrounds = xgb_cv$best_iteration, metrics = list("error"),
                    max_depth = 5, objective = "binary:logistic",
                    verbose = F)
```

```{r}
    pred_vec <- as.numeric(predict(xgb_opt, test_grid, type = "response") >= 0.5)
    plot_prediction_region(x, y, pred_vec, test_grid,
    xlab = "Dimension 1", ylab = "Dimension 2",
    main = paste("XGBoost decision boundary\n", xgb_cv$best_iteration, "trees, depth 3"),
    pch = 16, asp = T)
```

Here, we don't see the gaps we had earlier, and our model cannot replicate the smoothness of the 
curve of the original decision boundary 
